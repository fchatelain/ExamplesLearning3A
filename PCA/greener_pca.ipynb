{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GreEn-ER Dataset\n",
    "\n",
    "The data set consists of electrical consumption data for the GreEn-ER building, as well as meteorological data since September 2016. \n",
    "\n",
    "Here there is $p=28$ variables sampled every hour producing $n=16743$ samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "greener_dataset= sio.loadmat('greener_conso_meteo_2016_2018.mat')\n",
    "X= greener_dataset['data']\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables (available measurements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vbls= greener_dataset['variable_names']\n",
    "variable_names = [vbls[0][i][0] for i in range(vbls.shape[1])]\n",
    "print(variable_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vunits= greener_dataset['variable_units']\n",
    "variable_units = [vunits[0][i][0] for i in range(vunits.shape[1])]\n",
    "print(variable_units)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Biplot function to display both scores and loadings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def biplot(score,coeff,labels=None):\n",
    "    xs = score[:,0]\n",
    "    ys = score[:,1]\n",
    "    n=coeff.shape[1]\n",
    "    xvector = coeff[0] # see 'prcomp(my_data)$rotation' in R\n",
    "    yvector = coeff[1]\n",
    "    \n",
    "    scalex = 1.0/(xs.max()- xs.min())\n",
    "    scaley = 1.0/(ys.max()- ys.min())\n",
    "    plt.scatter(xs*scalex,ys*scaley)\n",
    "    for i in range(n):\n",
    "        plt.arrow(0, 0, xvector[i], yvector[i],color='r',alpha=0.5,width=0.0005, head_width=0.0025)\n",
    "        if labels is None:\n",
    "            plt.text(xvector[i]* 1.15, yvector[i] * 1.15, \"Var\"+str(i+1), color='g', ha='center', va='center')\n",
    "        else:\n",
    "            plt.text(xvector[i]* 1.15, yvector[i] * 1.15, labels[i], color='g', ha='center', va='center')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline example to perform PCA on real-word datasets (missing values, scaling, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of use on synthetic dummy data\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.impute import SimpleImputer\n",
    "#from sklearn.preprocessing import Imputer #deprecated\n",
    "import numpy as np\n",
    "\n",
    "# draw some data with independent variables!\n",
    "X=np.random.randn(100,10)\n",
    "\n",
    "# create dummy variable names\n",
    "vbls= []\n",
    "for i in range(10):\n",
    "    vbls.append( 'var' + str(i+1) )\n",
    "print(vbls)\n",
    "\n",
    "# add scale effect\n",
    "X[:,2]= 5 * X[:,2] ;\n",
    "\n",
    "# add missing data\n",
    "X[20:25,7]= np.nan\n",
    "\n",
    "\n",
    "# impute missing data\n",
    "#imp = Imputer(missing_values=np.nan, strategy='mean') # deprecated\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "X_imp= imp.fit_transform(X)\n",
    "\n",
    "# normalize\n",
    "scaler = StandardScaler()\n",
    "X_scale= scaler.fit_transform(X_imp)\n",
    "\n",
    "# make PCA\n",
    "pca = PCA()\n",
    "X_transform=pca.fit_transform(X_scale)\n",
    "\n",
    "# visualize the scores and loadings for the two principal compenents \n",
    "biplot(X_transform[:,0:2],pca.components_[0:2,:], labels=vbls)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # TODO\n",
    " \n",
    " 1. Restrict first to electrical consumption variables\n",
    "   - transform them in principal component basis\n",
    "   - visualize scores and loadings\n",
    "   - see the scale effects \n",
    "   - perform dimension reduction\n",
    " 2. Same on the full dataset: electrical consumption + meteorological data\n",
    "   - take care of normalization, missing data, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
